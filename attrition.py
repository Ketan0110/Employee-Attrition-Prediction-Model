# -*- coding: utf-8 -*-
"""attrition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17sNVy9eymfmDeyAiGAzQjxyAmsJG38eh
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import KFold,cross_validate
from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, confusion_matrix, precision_recall_curve, roc_curve,roc_auc_score,precision_recall_curve, auc, f1_score
from imblearn.over_sampling import SMOTE
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import Perceptron
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.naive_bayes import GaussianNB
import warnings
warnings.filterwarnings('ignore')
from sklearn.model_selection import train_test_split

from google.colab import drive
drive.mount('/content/drive')

"""display.max_columns function is used to print the specific number of attributes which is set to none so that no attribute is truncated while printing

**DATASET DESCRIPTION**
"""

dataset = pd.read_csv("/content/drive/MyDrive/original_dataset.csv")
# dataset = pd.read_csv("/content/original_dataset.csv")
pd.options.display.max_columns = None
dataset.shape
dataset.head()

# from google.colab import drive
# drive.mount('/content/drive')

# dataset = pd.read_csv("/content/drive/MyDrive/WA_Fn-UseC_-HR-Employee-Attrition.csv")
# pd.options.display.max_columns = None

# dataset.head()

dataset.shape

"""**CHECKING NULL VALUES FOR THE DATASET**"""

dataset.isnull().any()

"""**DESCRIBING THE DATASET**"""

features = dataset.columns

for feature in features:
    print(dataset[feature].value_counts())
    print("-------------------")

dataset.describe().T

"""**CHECKING FOR THE OUTLIERS USING BOXPLOT**"""

f, ax = plt.subplots(2,6,figsize = (25,25))
sns.boxplot(data = dataset['PercentSalaryHike'],ax = ax[0][0]).set_title('PercentSalaryHike')
sns.boxplot(data = dataset['TotalWorkingYears'],ax = ax[0][1]).set_title('TotalWorkingYears')
sns.boxplot(data = dataset['TrainingTimesLastYear'], ax = ax[0][2]).set_title('TrainingTimesLastYear')
sns.boxplot(data = dataset['YearsAtCompany'],ax = ax[0][3]).set_title('YearsAtCompany')
sns.boxplot(data = dataset['YearsInCurrentRole'],ax = ax[0][4]).set_title('YearsInCurrentRole')
sns.boxplot(data = dataset['YearsSinceLastPromotion'],ax = ax[0][5]).set_title('YearsSinceLastPromotion')
sns.boxplot(data = dataset['YearsWithCurrManager'],ax = ax[1][0]).set_title('YearsWithCurrManager')
sns.boxplot(data = dataset['Age'],ax = ax[1][1]).set_title('Age')
sns.boxplot(data = dataset['DailyRate'],ax = ax[1][2]).set_title('DailyRate')
sns.boxplot(data = dataset['DistanceFromHome'],ax = ax[1][3]).set_title('DistanceFromHome')
sns.boxplot(data = dataset['MonthlyIncome'],ax = ax[1][4]).set_title('MonthlyIncome')
sns.boxplot(data = dataset['NumCompaniesWorked'],ax = ax[1][5]).set_title('NumCompaniesWorked')

dataset.shape

"""**deleting the redundant columns**"""

#dataset.drop(['EmployeeNumber','EmployeeCount', 'Over18', 'StandardHours'] ,axis = 0,inplace=True)

columns_to_drop = ['EmployeeNumber', 'EmployeeCount', 'Over18', 'StandardHours']

# Check if columns exist in the dataset
missing_columns = [col for col in columns_to_drop if col not in dataset.columns]
if missing_columns:
    print(f"Error: Columns {missing_columns} not found in the dataset.")
else:
    # Drop specified columns
    dataset.drop(columns_to_drop, axis=1, inplace=True)
    print("Columns dropped successfully.")

"""**converting the columns into binary numbers**"""

dataset['Attrition'] = dataset['Attrition'].apply(lambda x: 1 if x == 'Yes' else 0)
dataset['OverTime'] = dataset['OverTime'].apply(lambda x: 1 if x == 'Yes' else 0)
dataset['Gender'] = dataset['Gender'].apply(lambda x: 1 if x == 'Male' else 0)

"""**Combining EnvironmentSatisfaction, JobInvolvement, JobSatisfaction, RelationshipSatisfaction to form a new Feature HolisticSatisfaction**"""

dataset['HolisticSatisfaction'] = dataset['EnvironmentSatisfaction'] + dataset['JobInvolvement'] + dataset['JobSatisfaction']+dataset['RelationshipSatisfaction']

dataset.drop(['JobInvolvement','JobSatisfaction','RelationshipSatisfaction','EnvironmentSatisfaction'],axis = 1)
print(dataset.shape)
dataset.head()

dataset.head()

"""**Dummy Variables are generated for categorical features**"""

dataset = pd.get_dummies(data = dataset, columns=['BusinessTravel','Department','JobRole','MaritalStatus','EducationField'])
dataset.head()

dataset.shape

cols = list(dataset.columns)
remove = []
dt = np.array(dataset)
thresh = 2.5

"""**Calculating ZScore and removing outliers from YearsAtCompany, TotalWorkingHours, and MonthlyIncome**"""

for col_idx in [13, 23, 26]:
    data = dt[:, col_idx]
    m = np.mean(data)
    s = np.std(data)
    for i in range(len(data)):
        z = abs((data[i] - m) / s)
        if z > thresh:
            remove.append(i)

# list of outliers
remove = list(set(remove))
remove.sort()

print("Initial Length of dataset ==> ", len(dt))

# Removing outliers from the dataset
for idx in sorted(remove, reverse=True):
    dt = np.delete(dt, idx, 0)
print("Final Length of dataset ==> ", len(dt))
print("No. of Outliers removed ==> ", len(remove))

dt = pd.DataFrame(dt)
dt.columns = cols
dataset = dt

features = dataset.columns

for feature in features:
    print(dataset[feature].value_counts())
    print("-------------------")

# # Plotting the distribution of the attrition
# sns.countplot(x ='Attrition',data = dataset)

"""**separating target variable and features**"""

df_x = dataset.drop(['Attrition'], axis=1)
df_y = dataset['Attrition']
df_x.shape
df_y.shape

df_y

mms = MinMaxScaler()
dt_x = df_x

dt_x[["Age", "DailyRate", "DistanceFromHome", "HourlyRate", "MonthlyRate", "MonthlyIncome", "NumCompaniesWorked", "PercentSalaryHike", "TotalWorkingYears", "TrainingTimesLastYear", "YearsAtCompany", "YearsWithCurrManager", "YearsInCurrentRole", "YearsSinceLastPromotion", "HolisticSatisfaction"]] = mms.fit_transform(dt[["Age", "DailyRate", "DistanceFromHome", "HourlyRate", "MonthlyRate", "MonthlyIncome", "NumCompaniesWorked", "PercentSalaryHike", "TotalWorkingYears", "TrainingTimesLastYear", "YearsAtCompany", "YearsWithCurrManager", "YearsInCurrentRole", "YearsSinceLastPromotion", "HolisticSatisfaction"]])
df_x = dt_x

dataset.to_csv('ml_cleaned_final.csv', index = False)

# features = df_x.columns

# for feature in features:
#     print(dataset[feature].value_counts())
#     print("-------------------")



# def classisfier(model,params, x, y):
#   model = model(**params)
#   # using the kfold to split the test train data
#   kfold = KFold(n_splits = 10)

#   # creating numpy arrray from dataframes
#   x = df_x.iloc[:,:].values
#   y = df_y.iloc[:].values
#   y = np.array([y]).T

#   #using cross validate to train the model and measure the performance metrics of trained model
#   scores = cross_validate(model,x,y,cv = kfold,scoring = ['accuracy','f1','precision','recall'])

#   #using average function of numpy library for the scores
#   accuracy = round(np.average(scores['test_accuracy'])*100,2)
#   precision = round(np.average(scores['test_precision'])*100,2)
#   recall = round(np.average(scores['test_recall'])*100,2)
#   f1_Score = round(np.average(scores['test_f1'])*100,2)

#   # Printing the  data
#   data = [[accuracy,precision,recall,f1_Score]]
#   print("Accuracy:", data[0][0])
#   print("Precision:", data[0][1])
#   print("Recall:", data[0][2])
#   print("F1 Score:", data[0][3])

# Dictionary to store the results
results = {
    'Model': [],
    'Accuracy': [],
    'Precision': [],
    'Recall': [],
    'F1 Score': [],
    'Confusion_Matrix': []
}

def classifier(model, params, x, y):
    # Initializing the model
    model = model(**params)

    # Initializing the kfold
    kfold = KFold(n_splits = 10)

    # Converting dataframes into numpy arrays
    x = df_x.iloc[:,:].values
    y = df_y.iloc[:].values
    y = np.array([y]).T
    y = y.ravel()
    # x = x.values
    # y = y.values.ravel()
    # print("X.shape = "+str(x.shape)+" || Y.shape = "+str(y.shape))
    # print()

    # initializing metric variables
    accuracy = 0
    precision = 0
    recall = 0
    f1_Score = 0

    # Initialize an empty confusion matrix (2x2 matrix for binary classification)
    aggregated_cm = np.zeros((2, 2))

    # Looping through the folds
    for train_idx, test_idx in kfold.split(x):

        # Generating test and training set
        x_train, x_test = x[train_idx], x[test_idx]
        y_train, y_test = y[train_idx], y[test_idx]

         # Initializing SMOTE with a fixed random state for reproducible results
        sm = SMOTE(random_state = 12)
        y_train = y_train.astype(int)
        # Transforming the training data
        x_train_sm, y_train_sm = sm.fit_resample(x_train, y_train)
        x_train_sm = x_train.astype(float)
        y_train_sm = y_train.astype(int)

        # Fitting the transformed data in the model
        x_train_sm = x_train_sm.astype(float)
        y_train_sm = y_train_sm.astype(int)
        model.fit(x_train_sm, y_train_sm)

        # Predicting the model on the test set
        y_test_pred = model.predict(x_test).astype(int)
        y_test = y_test.astype(int)

        cm = confusion_matrix(y_test_pred, y_test)
        aggregated_cm += cm
        # # Storing the metrics
        accuracy = accuracy + accuracy_score(y_test, y_test_pred)
        precision = precision + precision_score(y_test, y_test_pred)
        recall = recall + recall_score(y_test, y_test_pred)
        f1_Score = f1_Score + f1_score(y_test, y_test_pred)

   # Rounding off the metrics
    accuracy = round(accuracy*10,2)
    precision = round(precision*10,2)
    recall = round(recall*10,2)
    f1_Score = round(f1_Score*10,2)

    # Store the results
    results['Model'].append(model)
    results['Accuracy'].append(accuracy)
    results['Precision'].append(precision)
    results['Recall'].append(recall)
    results['F1 Score'].append(f1_Score)
    results['Confusion_Matrix'].append(aggregated_cm)
# Printing the  data

    print(aggregated_cm)
    data = [[accuracy,precision,recall,f1_Score]]
    print("Accuracy:", data[0][0])
    print("Precision:", data[0][1])
    print("Recall:", data[0][2])
    print("F1 Score:", data[0][3])

def returnData(model,params, x, y):

    model = model(**params)
    kfold = KFold(n_splits = 10)

    x = df_x.iloc[:,:].values
    y = df_y.iloc[:].values
    y = np.array([y]).T
    # print("X.shape = "+str(x.shape)+" || Y.shape = "+str(y.shape))

    x = x.astype(float)
    y = y.astype(int)
    scores = cross_validate(model,x,y,cv = kfold,scoring = ['accuracy','f1','precision','recall'])
    accuracy = round(np.average(scores['test_accuracy'])*100,2)
    precision = round(np.average(scores['test_precision'])*100,2)
    recall = round(np.average(scores['test_recall'])*100,2)
    f1_Score = round(np.average(scores['test_f1'])*100,2)

    data = [accuracy,precision,recall,f1_Score]
    return data

# from sklearn.metrics import confusion_matrix
# from sklearn.model_selection import KFold
# import numpy as np

# def returnData(model_class, params, x, y, n_splits=10):
#     # Initialize the model with given parameters
#     model = model_class(**params)

#     # Ensure x and y are numpy arrays
#     x = np.array(x)
#     y = np.array(y)

#     # Create KFold cross-validator
#     kfold = KFold(n_splits=n_splits, shuffle=True, random_state=27)

#     # Initialize an empty confusion matrix (2x2 matrix for binary classification)
#     aggregated_cm = np.zeros((2, 2))

#     # Initialize lists to store the average scores
#     accuracy = []
#     precision = []
#     recall = []
#     f1_Score = []

#     # Iterate through each fold
#     for train_index, test_index in kfold.split(x, y):
#         # Split data into training and testing sets
#         x_train, x_test = x[train_index].astype(float), x[test_index].astype(float)
#         y_train, y_test = y[train_index].astype(int), y[test_index].astype(int)

#         # Train the model
#         model.fit(x_train, y_train)

#         # Make predictions
#         y_pred = model.predict(x_test)

#         # Calculate and add confusion matrix for this fold
#         cm = confusion_matrix(y_test, y_pred)
#         aggregated_cm += cm

#         # Calculate and collect performance metrics
#         accuracy.append(model.score(x_test, y_test))
#         precision.append(precision_score(y_test, y_pred))
#         recall.append(recall_score(y_test, y_pred))
#         f1_Score.append(f1_score(y_test, y_pred))

#     # Calculate the average metrics across all folds
#     avg_accuracy = round(np.mean(accuracy) * 100, 2)
#     avg_precision = round(np.mean(precision) * 100, 2)
#     avg_recall = round(np.mean(recall) * 100, 2)
#     avg_f1_score = round(np.mean(f1_score) * 100, 2)

#     # Return the average metrics and the aggregated confusion matrix
#     return [avg_accuracy, avg_precision, avg_recall, avg_f1_score], aggregated_cm

# # Now, you can call the function and receive the performance metrics and confusion matrix

def makeGraphMD(model, df_x, df_y):

    precision = []
    recall = []
    f1 = []
    vals = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
    for i in vals:
        # print(i)
        data = returnData(model,{'random_state':27, 'max_depth':i},df_x,df_y)
        # print(data)
        precision.append(data[1])
        recall.append(data[2])
        f1.append(data[3])

    plt.figure()
    plt.plot(vals, precision, label = "Precision")
    plt.plot(vals, recall, label = "Recall")
    plt.plot(vals, f1, label = "F1-score")
    plt.xlabel("Depth")
    plt.ylabel("Values")
    plt.legend()
    plt.show()

classifier(GaussianNB,{},df_x,df_y)

classifier(LogisticRegression,{},df_x,df_y)

classifier(DecisionTreeClassifier,{},df_x,df_y)

classifier(RandomForestClassifier,{},df_x,df_y)

classifier(Perceptron,{},df_x,df_y)

classifier(GradientBoostingClassifier,{},df_x,df_y)

makeGraphMD(DecisionTreeClassifier, df_x, df_y)

makeGraphMD(RandomForestClassifier, df_x, df_y)

makeGraphMD(GradientBoostingClassifier, df_x, df_y)

# makeGraphMD(Perceptron, df_x, df_y)

print(results)

performance_df = pd.DataFrame(results)
print(performance_df)

# Set the model names as the index for easier plotting
performance_df.set_index('Model', inplace=True)

# Plot performance metrics for each model
performance_df.plot(kind='bar', figsize=(10, 6))
plt.title('Performance Metrics by Model')
plt.xlabel('Model')
plt.ylabel('Score')
plt.xticks(rotation=45)
plt.legend(title='Metric')
plt.grid(axis='y')
plt.tight_layout()
plt.show()

# Iterate through each model's confusion matrix
for index, row in performance_df.iterrows():
    # Get the model name and confusion matrix string
    model_name = row.name  # Index as model name
    conf_matrix_str = row['Confusion_Matrix']

    # Convert the confusion matrix string to a 2D NumPy array
    conf_matrix = np.array(conf_matrix_str).astype(int)

    # Plot the confusion matrix as a heatmap
    plt.figure(figsize=(6, 4))
    sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='g')

    # Add title and labels
    plt.title(f'Confusion Matrix - {model_name}')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')

    # Display the plot
    plt.show()
    print()

